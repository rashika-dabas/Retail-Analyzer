{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"BigDataAssignment\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sql = \"select * from classicmodels.customers\"\n",
    "sql2 = \"select * from classicmodels.orders\"\n",
    "database = \"classicmodels\"\n",
    "user = \"root\"\n",
    "password = \"Rashika19\"\n",
    "server = \"localhost\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Reading customers table\n",
    "df_customers = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "# Reading orders table\n",
    "df_orders = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql2) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "# Joining the above tables using customerNumber and then saving it\n",
    "df_data = df_customers.join(df_orders, \"customerNumber\")\n",
    "df_data.show()\n",
    "df_data_csv = df_data.toPandas()\n",
    "print(df_data_csv.head())\n",
    "df_data_csv.to_csv('customersfinal.csv', index=False)\n",
    "\n",
    "# Selecting only the city from joined table\n",
    "df_joined = df_data.select(df_customers[\"city\"])\n",
    "\n",
    "# Grouping by city and counting orders per city\n",
    "df_grouped = df_joined.groupBy(\"city\").count()\n",
    "\n",
    "# Displaying the grouped DataFrame\n",
    "df_grouped.show()\n",
    "\n",
    "# Saving the grouped Dataframe as data source 2\n",
    "datasource2 = df_grouped.toPandas()\n",
    "print(datasource2.head())\n",
    "datasource2.to_csv('datasource2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"BigDataAssignment\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading data\n",
    "customersDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/data/adventureworks/customersfinal.csv\")\n",
    "\n",
    "# Selecting only the city from table\n",
    "df_joined = customersDF.select(customersDF[\"city\"])\n",
    "\n",
    "# Grouping by city and counting orders per city\n",
    "groupedDF = df_joined.groupBy(\"city\").count()\n",
    "\n",
    "# Displaying the grouped dataframe\n",
    "groupedDF.show()\n",
    "\n",
    "# Saving the grouped dataframe as datasource 1\n",
    "datasource1 = groupedDF.toPandas()\n",
    "print(datasource1.head())\n",
    "datasource1.to_csv('datasource1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataAssignment\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://rashika19:hirashi@rashika.4jonn.mongodb.net\") \\\n",
    "    .config(\"spark.mongodb.input.database\", \"sample_supplies\") \\\n",
    "    .config(\"spark.mongodb.input.collection\", \"sales\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://rashika19:hirashi@rashika.4jonn.mongodb.net\") \\\n",
    "    .config(\"spark.mongodb.output.database\", \"sample_supplies\") \\\n",
    "    .config(\"spark.mongodb.output.collection\", \"sales\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading data from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "# Groupby customer using storeLocation\n",
    "df_grouped = df.groupBy(\"storeLocation\").count()\n",
    "\n",
    "# Displaying the first few records\n",
    "df_grouped.show()\n",
    "\n",
    "# Saving the DataFrame\n",
    "datasource3 = df_grouped.toPandas()\n",
    "print(datasource3.head())\n",
    "datasource3.to_csv('datasource3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataAssignment\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:7.4.1.jre8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Defining the JDBC connection properties\n",
    "jdbc_url = \"jdbc:sqlserver://rashikaserver.database.windows.net:1433;database=myfirstdatabase\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"azureuser\",\n",
    "    \"password\": \"Rashika19\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Reading data from Azure SQL Server\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"customers\", properties=jdbc_properties)\n",
    "\n",
    "# Displaying the first few records\n",
    "df.show()\n",
    "\n",
    "# Saving the dataframe\n",
    "datasource4 = df.toPandas()\n",
    "print(datasource4.head())\n",
    "datasource4.to_csv('datasource4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"BigDataAssignment\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading data sources\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/datasource1.csv\")\n",
    "\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/datasource2.csv\")\n",
    "\n",
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/datasource4.csv\")\n",
    "\n",
    "# Combining data sources\n",
    "combined_df = df1.union(df2).union(df3)\n",
    "\n",
    "# Displaying combined dataframe\n",
    "combined_df.show()\n",
    "\n",
    "# Saving combined dataframe\n",
    "Combined_df = combined_df.toPandas()\n",
    "print(Combined_df.head())\n",
    "Combined_df.to_csv('Combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"BigDataAssignment\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading combined dataframe and remove duplicates (to avoid redundancies after joining)\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/Combined_df.csv\")\n",
    "df1 = df1.dropDuplicates(['city']).withColumnRenamed('count','count_Combined_df')\n",
    "\n",
    "# Reading data source 3\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/datasource3.csv\")\n",
    "df2 = df2.withColumnRenamed(\"storeLocation\", \"city\").withColumnRenamed('count','count_datasource3')\n",
    "\n",
    "# Joining dataframes\n",
    "joined_df = df1.join(df2, 'city', 'inner')\n",
    "joined_df.show()\n",
    "\n",
    "# Saving result dataframe\n",
    "Result_df = joined_df.toPandas()\n",
    "print(Result_df.head())\n",
    "Result_df.to_csv('Result_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataAssignment\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:7.4.1.jre8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading CSV file into dataframe\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/azureuser/Result_df.csv\")\n",
    "\n",
    "# Displaying dataframe\n",
    "df.show()\n",
    "\n",
    "# Defining the JDBC connection properties\n",
    "jdbc_url = \"jdbc:sqlserver://rashikaserver.database.windows.net:1433;database=myfirstdatabase\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"azureuser\",\n",
    "    \"password\": \"Rashika19\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Writing dataframe to Azure SQL Server table\n",
    "df.write.jdbc(url=jdbc_url, table=\"results\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
